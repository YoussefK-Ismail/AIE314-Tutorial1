{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# AIE314 Tutorial 1: Preprocessing Unstructured Data for LLM Applications\n",
    "**Team Member:** Youssef Khaled Ismail (ID: 22101260)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF python-docx openpyxl sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_md",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import fitz                          # PyMuPDF  â†’ PDF\n",
    "from docx import Document            # python-docx â†’ DOCX\n",
    "import openpyxl                      # openpyxl  â†’ XLSX\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print('All libraries imported successfully âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_md",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 3: Text Extraction (PDF / DOCX / XLSX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    pages_text = [page.get_text() for page in doc]\n",
    "    doc.close()\n",
    "    return '\\n'.join(pages_text)\n",
    "\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract all paragraph text from a DOCX file.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "    return '\\n'.join(paragraphs)\n",
    "\n",
    "\n",
    "def extract_text_from_xlsx(file_path: str) -> str:\n",
    "    \"\"\"Extract all cell values from every sheet in an XLSX file.\"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    all_text = []\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "        all_text.append(f'[Sheet: {sheet_name}]')\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            row_text = ' | '.join(str(cell) for cell in row if cell is not None)\n",
    "            if row_text.strip():\n",
    "                all_text.append(row_text)\n",
    "    return '\\n'.join(all_text)\n",
    "\n",
    "\n",
    "def extract_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Router function â€” detects file extension and calls the right extractor.\n",
    "    Supported: .pdf  |  .docx  |  .xlsx\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    if ext == '.pdf':\n",
    "        print(f'  â†’ Extracting PDF: {file_path}')\n",
    "        return extract_text_from_pdf(file_path)\n",
    "\n",
    "    elif ext == '.docx':\n",
    "        print(f'  â†’ Extracting DOCX: {file_path}')\n",
    "        return extract_text_from_docx(file_path)\n",
    "\n",
    "    elif ext == '.xlsx':\n",
    "        print(f'  â†’ Extracting XLSX: {file_path}')\n",
    "        return extract_text_from_xlsx(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file type: \"{ext}\". Supported: .pdf, .docx, .xlsx')\n",
    "\n",
    "print('Extraction functions defined âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalize_md",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 4: Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize raw extracted text:\n",
    "      1. Unicode normalization (NFKC)\n",
    "      2. Lowercase\n",
    "      3. Remove special/control characters\n",
    "      4. Collapse multiple whitespace / blank lines\n",
    "      5. Strip leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    # 1. Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # 2. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Remove non-printable / control characters (keep newlines)\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)         # collapse horizontal whitespace\n",
    "    text = re.sub(r'[^\\x20-\\x7eØ£-ÙŠ\\n]', '', text)  # keep ASCII printable + Arabic + newlines\n",
    "\n",
    "    # 4. Collapse multiple blank lines into a single newline\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "\n",
    "    # 5. Strip\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "print('Normalization function defined âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk_md",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 5: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping word-level chunks.\n",
    "\n",
    "    Args:\n",
    "        text       : normalized text string\n",
    "        chunk_size : max number of words per chunk\n",
    "        overlap    : number of words shared between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys: chunk_id, text, word_count\n",
    "    \"\"\"\n",
    "    words  = text.split()\n",
    "    chunks = []\n",
    "    start  = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end        = min(start + chunk_size, len(words))\n",
    "        chunk_text = ' '.join(words[start:end])\n",
    "        chunks.append({\n",
    "            'chunk_id'  : chunk_id,\n",
    "            'text'      : chunk_text,\n",
    "            'word_count': end - start\n",
    "        })\n",
    "        chunk_id += 1\n",
    "        start     = end - overlap   # slide window with overlap\n",
    "        if start >= end:            # prevent infinite loop on small texts\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "print('Chunking function defined âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embed_md",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Step 6: Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a lightweight multilingual model (works for Arabic + English)\n",
    "EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f'Model loaded: {EMBEDDING_MODEL} âœ…')\n",
    "\n",
    "\n",
    "def embed_chunks(chunks: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Add a semantic embedding vector to each chunk dict.\n",
    "\n",
    "    Args:\n",
    "        chunks : list of chunk dicts (from chunk_text)\n",
    "\n",
    "    Returns:\n",
    "        Same list with an extra key 'embedding' (list of floats)\n",
    "    \"\"\"\n",
    "    texts      = [c['text'] for c in chunks]\n",
    "    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "    for chunk, emb in zip(chunks, embeddings):\n",
    "        chunk['embedding'] = emb.tolist()   # convert numpy â†’ list for JSON serialization\n",
    "\n",
    "    return chunks\n",
    "\n",
    "print('Embedding function defined âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_md",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 7: Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path: str, output_dir: str = 'output',\n",
    "                 chunk_size: int = 300, overlap: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Full pipeline: extract â†’ normalize â†’ chunk â†’ embed â†’ save JSON.\n",
    "\n",
    "    Args:\n",
    "        file_path  : path to .pdf / .docx / .xlsx file\n",
    "        output_dir : folder to save the resulting JSON\n",
    "        chunk_size : words per chunk\n",
    "        overlap    : overlap words between chunks\n",
    "\n",
    "    Returns:\n",
    "        Result dict (also saved as JSON in output_dir)\n",
    "    \"\"\"\n",
    "    print(f'\\n{'='*60}')\n",
    "    print(f'Processing: {file_path}')\n",
    "\n",
    "    # 1. Extract\n",
    "    raw_text = extract_text(file_path)\n",
    "    print(f'  Raw text length: {len(raw_text)} chars')\n",
    "\n",
    "    # 2. Normalize\n",
    "    clean_text = normalize_text(raw_text)\n",
    "    print(f'  Normalized text length: {len(clean_text)} chars')\n",
    "\n",
    "    # 3. Chunk\n",
    "    chunks = chunk_text(clean_text, chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f'  Total chunks: {len(chunks)}')\n",
    "\n",
    "    # 4. Embed\n",
    "    print('  Generating embeddings...')\n",
    "    chunks_with_embeddings = embed_chunks(chunks)\n",
    "\n",
    "    # 5. Build result\n",
    "    file_name = os.path.basename(file_path)\n",
    "    result = {\n",
    "        'source_file' : file_name,\n",
    "        'total_chunks': len(chunks_with_embeddings),\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'chunks'      : chunks_with_embeddings\n",
    "    }\n",
    "\n",
    "    # 6. Save JSON\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name   = os.path.splitext(file_name)[0]\n",
    "    output_path = os.path.join(output_dir, f'{base_name}_processed.json')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f'  âœ… Saved â†’ {output_path}')\n",
    "    return result\n",
    "\n",
    "\n",
    "print('Pipeline function defined âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_md",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Step 8: Run on Your Files\n",
    "\n",
    "> **Google Colab:** Upload your files first using the Files panel (ğŸ“) on the left,  \n",
    "> then update `FILES` below with the correct paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Update these paths to match your uploaded files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILES = [\n",
    "    'computer_science_overview.pdf',\n",
    "    'computer_science_overview.docx',\n",
    "    'ExcelSample.xlsx',\n",
    "]\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "results = {}\n",
    "for file_path in FILES:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'âš ï¸  File not found: {file_path} â€” skipping.')\n",
    "        continue\n",
    "    result = process_file(file_path, output_dir='output', chunk_size=300, overlap=50)\n",
    "    results[file_path] = result\n",
    "\n",
    "print('\\nâœ… All files processed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_md",
   "metadata": {},
   "source": [
    "## ğŸ” Step 9: Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, result in results.items():\n",
    "    print(f\"\\n{'â”€'*50}\")\n",
    "    print(f\"File       : {result['source_file']}\")\n",
    "    print(f\"Chunks     : {result['total_chunks']}\")\n",
    "    print(f\"Emb. model : {result['embedding_model']}\")\n",
    "    print(f\"Emb. dim   : {len(result['chunks'][0]['embedding'])}\")\n",
    "    print(f\"\\nFirst chunk preview:\")\n",
    "    print(result['chunks'][0]['text'][:300], '...')"
   ]
  }
 ]
}"
